{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (1.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from pandas) (1.18.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Collecting html5lib\n",
      "  Downloading html5lib-1.0.1-py2.py3-none-any.whl (117 kB)\n",
      "\u001b[K     |████████████████████████████████| 117 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from html5lib) (1.14.0)\n",
      "Requirement already satisfied: webencodings in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from html5lib) (0.5.1)\n",
      "Installing collected packages: html5lib\n",
      "Successfully installed html5lib-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[38;5;1m✘ No compatible model found for 'es_core_web_sm' (spaCy v2.2.4).\u001b[0m\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (40.6.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: thinc==7.4.0 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.45.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (0.0.24)\n",
      "Collecting inflect\n",
      "  Downloading inflect-4.1.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: textsearch in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from contractions) (0.0.17)\n",
      "Requirement already satisfied: Unidecode in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from textsearch->contractions) (1.1.1)\n",
      "Requirement already satisfied: pyahocorasick in /home/carlosb/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages (from textsearch->contractions) (1.4.0)\n",
      "Installing collected packages: inflect\n",
      "Successfully installed inflect-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/carlosb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/carlosb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/carlosb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import html5lib\n",
    "import lxml\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cleantext():\n",
    "    \n",
    "    def __init__(self, text = \"test\"):\n",
    "        self.text = text\n",
    "        \n",
    "    def strip_html(self):\n",
    "        soup = BeautifulSoup(self.text, \"html.parser\")\n",
    "        self.text = soup.get_text()\n",
    "        return self\n",
    "\n",
    "    def remove_between_square_brackets(self):\n",
    "        self.text = re.sub('\\[[^]]*\\]', '', self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_numbers(self):\n",
    "        self.text = re.sub('[-+]?[0-9]+', '', self.text)\n",
    "        return self\n",
    "\n",
    "    def replace_contractions(self):\n",
    "        \"\"\"Replace contractions in string of text\"\"\"\n",
    "        self.text = contractions.fix(self.text)\n",
    "        return self\n",
    "    \n",
    "    def get_words(self):\n",
    "        self.words = nltk.word_tokenize(self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_non_ascii(self):\n",
    "        \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in self.words:\n",
    "            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_words.append(new_word)\n",
    "        self.words = new_words\n",
    "        return self\n",
    "\n",
    "    def to_lowercase(self):\n",
    "        \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in self.words:\n",
    "            new_word = word.lower()\n",
    "            new_words.append(new_word)\n",
    "        self.words = new_words\n",
    "        return self\n",
    "\n",
    "    def remove_punctuation(self):\n",
    "        \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in self.words:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "        self.words = new_words\n",
    "        return self\n",
    "\n",
    "    def replace_numbers(self):\n",
    "        \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "        p = inflect.engine()\n",
    "        new_words = []\n",
    "        for word in self.words:\n",
    "            if word.isdigit():\n",
    "                new_word = p.number_to_words(word)\n",
    "                new_words.append(new_word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        self.words = new_words\n",
    "        return self\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "        \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in self.words:\n",
    "            if word not in stopwords.words('english'):\n",
    "                new_words.append(word)\n",
    "        self.words = new_words\n",
    "        return self\n",
    "\n",
    "    def stem_words(self):\n",
    "        \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "        stemmer = LancasterStemmer()\n",
    "        stems = []\n",
    "        for word in self.words:\n",
    "            stem = stemmer.stem(word)\n",
    "            stems.append(stem)\n",
    "        self.words = stems\n",
    "        return self\n",
    "\n",
    "    def lemmatize_verbs(self):\n",
    "        \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmas = []\n",
    "        for word in self.words:\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "            lemmas.append(lemma)\n",
    "        self.words = lemmas\n",
    "        return self\n",
    "    \n",
    "    def join_words(self):\n",
    "        self.words = ' '.join(self.words)\n",
    "        return self\n",
    "    def spacy_tokenize(self):\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        nlp.remove_pipe('parser')\n",
    "        nlp.remove_pipe('ner')\n",
    "        nlp.remove_pipe('tagger')\n",
    "        # Replace all newlines and blanklines with special strings\n",
    "        regx = re.compile(r\"\\n\")\n",
    "        self.text = regx.sub(repl=\" \", string=self.text)\n",
    "        regx = re.compile(r\"\\n\\n\")\n",
    "        self.text = regx.sub(repl=\" \", string=self.text)\n",
    "\n",
    "        # Make all white space a single space\n",
    "        regx = re.compile(r\"\\s+\")\n",
    "        self.text = regx.sub(repl=\" \", string=self.text)\n",
    "\n",
    "        # Remove any trailing or leading white space\n",
    "        self.text = self.text.strip(\" \")\n",
    "        \n",
    "        self.words = [token.lower_ for token in  list(nlp.pipe([self.text], n_threads=1))[0]]\n",
    "        return self\n",
    "    \n",
    "    def do_all(self, text,spacy=False):\n",
    "        \n",
    "        self.text = text\n",
    "        self = self.strip_html()\n",
    "        self = self.remove_numbers()\n",
    "        self = self.replace_contractions()\n",
    "        if spacy:\n",
    "            self = self.spacy_tokenize()\n",
    "        else:\n",
    "            self = self.get_words()\n",
    "        self = self.remove_punctuation()\n",
    "        self = self.remove_non_ascii()\n",
    "        self = self.remove_stopwords()\n",
    "        self = self.stem_words()\n",
    "        self = self.lemmatize_verbs()\n",
    "        self = self.join_words()\n",
    "        \n",
    "        return self.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open('./test_html_article')\n",
    "html = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prepar est irresist sandwich cubanito es as de facil hoy inter carn cer fruta fruto seco legumbr pescado tuberculo verdura mas secc al minuto internac politic vid deport econom opin gent cultur suceso participac blog rcd espanyol europ fact check suscrib tema can la contr nat big vang salud tecnolog televid sery muy fan cribeo magazin lifestyl vias vivo mot de mod com vivo seguro can andorr el comprad histor vid ed loc madrid barcelon cataluny andaluc comunidad valencian pai vasco ver mas suplemento cultura vanguard dossy vanguard grand tema libro de vanguard clasificado monografico can relod servicio edicio en catal ed impres hemerotec bols programac tv loteria horoscopo juego carteler agend juny report rss necrologica esquela gourmet la vanguard lv shop entrada de vanguard pista de esqu header_logo_text al dia opin sitio tendencia mater prim receta de carn hueso video receta prepar est irresist sandwich cubanito es as de facil un plato sencillo pero muy apetitoso prepar est apetitos tart de fresa es as de facil video com actualizado est seman eugen de diego nos ensen prepar un recet facil sencill muy apetitos un sandwich cubanito de primer calidad con el que conseguira sorprend cualquy afortunado que lo prueb qui sab como sigu la recet que encontrara continuac cubanito ingredy g de carn de cerdo sal pimient naranj de zumo g aceit de oliv barr de pan tipo baguet vien g queso suizo g jamon york g pepinillo encurtido g mostaz savor mantequill la recet elaborac dispon la carn en un bandej salpiment exprimir zumo de naranj sobr la carn roci la carn con aceit de oliv hornear dur hor c cort la carn en trozo delgado cort la baguet la mitad en un de las mitad dispon el queso los trozo de carn el jamon york los pepinillo en la otr mitad de la baguet dispon la mostaz gusto cerr el bocadillo calent la planch cubrir con mantequill planch el bocadillo cort el bocadillo por la mitad servir al minuto coronavir en gijon ultim hor de los afectado de la desescalad en asturia trump se muestr contento por la vuelt de kim jong un qui bien ter steg estoy muy fel en barcelon arrimada llam que el coronavir sea un oportunidad par el nacionalismo los fallecido en hora desciend hast en espan que tipo de mascarill necesito segun mi situac leer comentario collectionmetaeyjexaioijkvqilcjhbgcioijiuzinijeyjexblijoibglzwnvbwlbnrziiwiyxjawnszulkijoimmmwmtdkmdgtnjuxnsxmwvhltgwngitmtdkyzqwmjcmwviiiwidglbguioijqcmvwyxjhciblcrliglycmvzaxnawjszsbzxhuwmguxbmrawnoignymfuaxrvigvzifxmjaxoefzxhuwmgvkigrligzcdtawztfjawxcdtiwmtkilcjcmwioijodhrwczpclwvddlmxhdmfuzvhcmrpysjbclnvbwvyxcyzwnldgfzxcymdiwmdmxnvwvndcmtamjgymzkxxczywkdljacjdwjhbmlbyhcktzgutzmfjawwtymjywrpbgxvlxnvzmlzdgljywrvlwvzvuasodgsiiwidgfncyiwyjzdgyesisilwvytzxiilcjsyvzhbmdyxjkaweixswiaxnzijoidxjuomxpdmvmexjlomdydxbvzkbzeuznlyzsjbzpzaxrlptmmtexmijxekfcqq fryzmxzlhgupjvvvbdvegaumrtztchecksumdcfcdeeafefballowcommentallowednetidcdeabdcebnetworknamegrupogodofyrecositeid com receta la vanguard ed todo los derecho reservado qui somo contacto aviso leg ayud politic de cooky otra web politic de privacidad be de privacidad'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleantext().do_all(html,spacy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tagger', <spacy.pipeline.pipes.Tagger at 0x7fb9304db460>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hoy interesa carnes cereales frutas frutos secos legumbres pescados tubérculos verduras mas secciones al minuto internacional política vida deportes economía opinión gente cultura sucesos participación blogs rcd espanyol europa fact checking suscriptores temas canales la contra natural big vang salud tecnología televisión series muy fan cribeo magazine lifestyle viajes vivo motor de moda comer vivo seguro canal andorra el comprador historia y vida ediciones locales madrid barcelona catalunya andalucía comunidad valenciana país vasco ver más suplementos cultura|s vanguardia dossier vanguardia grandes temas libros de vanguardia clasificados monográficos canal relojes servicios edició en català edición impresa hemeroteca bolsa programación tv loterías horóscopo juegos cartelera agenda junior report rss necrológicas y esquelas gourmet la vanguardia lv shopping entradas de vanguardia pistas de esquí header_logo_text al día opinión sitios tendencias materia prima recetas de carne y hueso videos recetas preparar este irresistible sándwich cubanito es ‘ así de fácil ’ un plato sencillo pero muy apetitoso preparar esta apetitosa tarta de fresas es ‘ así de fácil ’ video comer actualizado a 16/03/2020 10:23 esta semana eugeni de diego nos enseña a preparar una receta fácil , sencilla y muy apetitosa . un sándwich cubanito de primera calidad con el que conseguirás sorprender a cualquier afortunado que lo pruebe . ¿ quieres saber cómo ? sigue la receta que encontrarás a continuación . cubanito ingredientes > 200 g de carne de cerdo > sal y pimientare > 1 naranja de zumo > 50 g aceite de oliva > 1 barra de pan tipo baguette o viena > 80 g queso suizo > 70 g jamón york > 30 g pepinillo encurtido > 20 g mostaza savora > mantequilla la receta elaboración 1 . disponer la carne en una bandeja , salpimentar . 2 . exprimir zumo de naranja sobre la carne . 3 . rociar la carne con aceite de oliva . hornear durante 1 hora a 180 º c. 4 . cortar la carne en trozos delgados . 5 . cortar la baguette a la mitad , en una de las mitades disponer el queso , los trozos de carne , el jamón york y los pepinillos . 6 . en la otra mitad de la baguette , disponer la mostaza a gusto y cerrar el bocadillo . 7 . calentar la plancha . cubrir con mantequilla y planchar el bocadillo . 8 . cortar el bocadillo por la mitad y servir . al minuto coronavirus en gijón : última hora de los afectados y de la desescalada en asturias trump se muestra “ contento ” por la vuelta de kim jong un , a quien ve “ bien ” ter stegen : “ estoy muy feliz en barcelona ” arrimadas llama a que el coronavirus “ no sea una oportunidad para el nacionalismo ” los fallecidos en 24 horas descienden hasta 164 en españa ¿ qué tipo de mascarilla necesito según mi situación ? leer comentarios { \" collectionmeta\":\"eyj0exaioijkv1qilcjhbgcioijiuzi1nij9.eyj0exblijoibgl2zwnvbw1lbnrziiwiyxj0awnszulkijoimmmwmtdkmdgtnjuxns0xmwvhltgwngitmtdkyzqwmjc2mwviiiwidgl0bguioijqcmvwyxjhciblc3rliglycmvzaxn0awjszsbzxhuwmguxbmr3awnoign1ymfuaxrvigvzifx1mjaxoefzxhuwmgvkigrligzcdtawztfjawxcdtiwmtkilcj1cmwioijodhrwczpcl1wvd3d3lmxhdmfuz3vhcmrpys5jb21cl2nvbwvyxc9yzwnldgfzxc8ymdiwmdmxnvwvndc0mta3mjgymzkxxc9zyw5kd2ljac1jdwjhbml0by1hc2ktzgutzmfjawwtym9jywrpbgxvlxnvzmlzdgljywrvlwv1z2vuas5odg1siiwidgfncyi6wyjzdg9yesisilwvy29tzxiilcjsyvzhbmd1yxjkaweixswiaxnzijoidxjuomxpdmvmexjlomdydxbvz29kbzeuznlyzs5jbzpzaxrlptm1mtexmij9.xekf0cqq -- fryz59mxzlhgupjvv4vbdvegaum8rtzt8\",\"checksum\":\"1dc942fcd45564e702eaf236e36fb286\",\"allowcomment\":\"allowed\",\"netid\":\"2c017d08 - 6515 - 11ea-804b-17dc402761eb\",\"networkname\":\"grupogodo1.fyre.co\",\"siteid\":351112 } comer recetas © la vanguardia ediciones todos los derechos reservados quiénes somos contacto aviso legal ayuda política de cookies otras webs política de privacidad área de privacidad'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "model = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No valid number words found! Please enter a valid number word (eg. two million twenty three thousand and forty nine)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-7af2fe1a798c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mword2number\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mw2n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisible_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw2n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NUM'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# result: [3, cups, of, coffee]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-7af2fe1a798c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mword2number\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mw2n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisible_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw2n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NUM'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# result: [3, cups, of, coffee]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bot_news_py38/lib/python3.8/site-packages/word2number/w2n.py\u001b[0m in \u001b[0;36mword_to_num\u001b[0;34m(number_sentence)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;31m# Error message if the user enters invalid input!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_numbers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No valid number words found! Please enter a valid number word (eg. two million twenty three thousand and forty nine)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m# Error if user enters million,billion, thousand or decimal point twice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No valid number words found! Please enter a valid number word (eg. two million twenty three thousand and forty nine)"
     ]
    }
   ],
   "source": [
    "from word2number import w2n\n",
    "doc = nlp(visible_text)\n",
    "tokens = [w2n.word_to_num(token.text) if token.pos_ == 'NUM' else token for token in doc]\n",
    "\n",
    "print(tokens) # result: [3, cups, of, coffee]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', 'Hoy', 'interesar', '\\n\\n', 'Carnes', '\\n\\n\\n', 'Cereales', '\\n\\n\\n', 'Frutas', '\\n\\n\\n', 'Frutos', 'seco', '\\n\\n\\n', 'Legumbres', '\\n\\n\\n', 'Pescados', '\\n\\n\\n', 'Tubérculos', '\\n\\n\\n', 'Verduras', '\\n\\n\\n\\n\\n\\n', 'Mas', '\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', 'Secciones', '\\n\\n\\n\\n', 'Al', 'Minuto', '\\n\\n\\n', 'Internacional', '\\n\\n\\n', 'Política', '\\n\\n\\n', 'Vida', '\\n\\n\\n', 'Deportes', '\\n\\n\\n', 'Economía', '\\n\\n\\n', 'Opinión', '\\n\\n\\n', 'Gente', '\\n\\n\\n', 'Cultura', '\\n\\n\\n', 'Sucesos', '\\n\\n\\n', 'Participación', '\\n\\n\\n', 'Blogs', '\\n\\n\\n', 'RCD', 'Espanyol', '\\n\\n\\n', 'Europa', 'Fact', 'Checking', '\\n\\n\\n', 'Suscriptores', '\\n\\n\\n', 'Temas', '\\n\\n\\n\\n\\n\\n', 'Canales', '\\n\\n\\n\\n', 'La', 'Contra', '\\n\\n\\n', 'Natural', '\\n\\n\\n', 'Big', 'Vang', '\\n\\n\\n', 'Salud', '\\n\\n\\n', 'Tecnología', '\\n\\n\\n', 'Televisión', '\\n\\n\\n', 'Series', '\\n\\n\\n', 'Muy', 'Fan', '\\n\\n\\n', 'Cribeo', '\\n\\n\\n', 'Magazine', 'Lifestyle', '\\n\\n\\n', 'Viajes', '\\n\\n\\n', 'Vivo', '\\n\\n\\n', 'Motor', '\\n\\n\\n', 'De', 'Moda', '\\n\\n\\n', 'Comer', '\\n\\n\\n', 'Vivo', 'Seguro', '\\n\\n\\n', 'Canal', 'Andorra', '\\n\\n\\n', 'El', 'Comprador', '\\n\\n\\n', 'Historia', 'y', 'Vida', '\\n\\n\\n\\n\\n\\n', 'Ediciones', 'local', '\\n\\n\\n\\n', 'Madrid', '\\n\\n\\n', 'Barcelona', '\\n\\n\\n', 'Catalunya', '\\n\\n\\n', 'Andalucía', '\\n\\n\\n', 'Comunidad', 'Valenciana', '\\n\\n\\n', 'País', 'Vasco', '\\n\\n\\n', 'Ver', 'más', '\\n\\n\\n\\n\\n\\n', 'Suplementos', '\\n\\n\\n\\n', 'Cultura|S', '\\n\\n\\n', 'Vanguardia', 'Dossier', '\\n\\n\\n', 'Vanguardia', 'Grandes', 'Temas', '\\n\\n\\n', 'Libros', 'de', 'Vanguardia', '\\n\\n\\n', 'Clasificados', '\\n\\n\\n', 'Monográficos', '\\n\\n\\n', 'Canal', 'Relojes', '\\n\\n\\n\\n\\n\\n', 'Servicios', '\\n\\n\\n\\n\\n\\n', 'Edició', 'en', 'català', '\\n\\n\\n', 'Edición', 'Impresa', '\\n\\n\\n', 'Hemeroteca', '\\n\\n\\n', 'Bolsa', '\\n\\n\\n', 'Programación', 'TV', '\\n\\n\\n', 'Loterías', '\\n\\n\\n', 'Horóscopo', '\\n\\n\\n', 'Juegos', '\\n\\n\\n\\n\\n\\n\\n', 'Cartelera', '\\n\\n\\n', 'Agenda', '\\n\\n\\n', 'Junior', 'Report', '\\n\\n\\n', 'RSS', '\\n\\n\\n', 'Necrológicas', 'y', 'esquela', '\\n\\n\\n', 'Gourmet', 'La', 'Vanguardia', '\\n\\n\\n', 'LV', 'Shopping', '\\n\\n\\n', 'Entradas', 'de', 'Vanguardia', '\\n\\n\\n', 'Pistas', 'de', 'esquí', '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', 'header_logo_text', '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', 'Al', 'día', '\\n\\n\\n', 'Opinión', '\\n\\n\\n', 'sitio', '\\n\\n\\n', 'tendencia', '\\n\\n\\n', 'Materia', 'Prima', '\\n\\n\\n', 'recetar', '\\n\\n\\n', 'de', 'carne', 'y', 'hueso', '\\n\\n\\n', 'videos', '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', 'Recetas', '\\n\\n\\n', 'Preparar', 'este', 'irresistible', 'sándwich', 'cubanito', 'ser', '‘', 'Así', 'de', 'fácil', '’', '\\n\\n\\n', 'Un', 'plato', 'sencillo', 'pero', 'muy', 'apetitoso', '\\n', 'Preparar', 'este', 'apetitoso', 'tarta', 'de', 'fresar', 'ser', '‘', 'Así', 'de', 'fácil', '’', '\\n\\n\\n\\n\\n\\n\\n\\n ', 'Video', '\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', 'COMER', '\\n\\n\\n', 'Actualizado', 'a', ' ', '16/03/2020', '10:23', '\\n\\n\\n\\n\\n', 'Esta', 'semana', 'Eugeni', 'de', 'Diego', 'no', 'enseñar', 'a', 'preparar', 'uno', 'recetar', 'fácil', ',', 'sencillo', 'y', 'muy', 'apetitoso', '.', 'Un', 'sándwich', 'cubanito', 'de', 'primero', 'calidad', 'con', 'el', 'que', 'conseguir', 'sorprender', 'a', 'cualquiera', 'afortunado', 'que', 'el', 'probar', '.', '¿', 'Quieres', 'saber', 'cómo', '?', 'Sigue', 'lo', 'recetar', 'que', 'encontrar', 'a', 'continuación', '.', '\\n\\n', 'Cubanito', '\\n', 'Ingredientes', '>', '200', 'gramo', 'de', 'carne', 'de', 'cerdo', '\\n', '>', 'salir', 'y', 'pimientare', '\\n', '>', '1', ' ', 'naranja', '\\n', 'de', 'zumo', '\\n', '>', '50', 'gramo', 'aceitar', 'de', 'olivar', '\\n', '>', '1', 'barrer', 'de', 'pan', 'tipo', 'baguette', 'o', 'viena', '\\n', '>', '80', 'gramo', 'queso', 'suizo', '\\n', '>', '70', 'gramo', 'jamón', 'York', '\\n', '>', '30', 'gramo', 'pepinillo', 'encurtir', '\\n', '>', '20', 'gramo', 'mostaza', 'Savora', '\\n', '>', 'Mantequilla', '\\n\\n', 'La', 'recetar', '\\n', 'Elaboración', '\\n\\n\\n\\n\\n', '1', '.', 'Disponer', 'lo', 'carne', 'en', 'uno', 'bandeja', ',', 'salpimentar', '.', '\\n', '2', '.', 'Exprimir', 'zumo', 'de', 'naranja', 'sobrar', 'lo', 'carne', '.', '\\n', '3', '.', 'Rociar', 'lo', 'carne', 'con', 'aceitar', 'de', 'olivar', '.', 'Hornear', 'durante', '1', 'hora', 'a', '180', 'º', 'C.', '\\n', '4', '.', 'Cortar', 'lo', 'carne', 'en', 'trozo', 'delgado', '.', '\\n', '5', '.', 'Cortar', 'lo', 'baguette', 'a', 'lo', 'mitad', ',', 'en', 'uno', 'de', 'los', 'mitad', 'disponer', 'el', 'queso', ',', 'lo', 'trozo', 'de', 'carne', ',', 'el', 'jamón', 'york', 'y', 'lo', 'pepinillo', '.', '\\n', '6', '.', 'En', 'lo', 'otro', 'mitad', 'de', 'lo', 'baguette', ',', 'disponer', 'lo', 'mostaza', 'a', 'gustar', 'y', 'cerrar', 'el', 'bocadillo', '.', '\\n', '7', '.', 'Calentar', 'lo', 'planchar', '.', 'Cubrir', 'con', 'mantequilla', 'y', 'planchar', 'el', 'bocadillo', '.', '\\n', '8', '.', 'Cortar', 'el', 'bocadillo', 'por', 'lo', 'mitad', 'y', 'servir', '.', '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', 'Al', 'minutar', '\\n', 'Coronavirus', 'en', 'Gijón', ':', 'Última', 'hora', 'de', 'lo', 'afectar', 'y', 'de', 'lo', 'desescalada', 'en', 'Asturias', '\\n', 'Trump', 'se', 'mostrar', '“', 'contentar', '”', 'por', 'lo', 'volver', 'de', ' ', 'Kim', 'Jong', 'uno', ',', 'a', 'quien', 'ver', '“', 'bien', '”', '\\n', 'Ter', 'Stegen', ':', '“', 'Estoy', 'muy', 'feliz', 'en', 'Barcelona', '”', '\\n', 'Arrimadas', 'llamar', 'a', 'que', 'el', 'coronavirus', '“', 'no', 'ser', 'uno', 'oportunidad', 'parir', 'el', 'nacionalismo', '”', '\\n', 'Los', 'fallecido', 'en', '24', 'hora', 'descender', 'hasta', '164', 'en', 'España', '\\n', '¿', 'Qué', 'tipo', 'de', 'mascarilla', 'necesitar', 'según', 'mi', 'situación', '?', '\\n\\n\\n\\n\\n\\n\\n', 'Leer', 'comentario', '\\n\\n\\n', '{', '\"', 'collectionMeta\":\"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ0eXBlIjoibGl2ZWNvbW1lbnRzIiwiYXJ0aWNsZUlkIjoiMmMwMTdkMDgtNjUxNS0xMWVhLTgwNGItMTdkYzQwMjc2MWViIiwidGl0bGUiOiJQcmVwYXJhciBlc3RlIGlycmVzaXN0aWJsZSBzXHUwMGUxbmR3aWNoIGN1YmFuaXRvIGVzIFx1MjAxOEFzXHUwMGVkIGRlIGZcdTAwZTFjaWxcdTIwMTkiLCJ1cmwiOiJodHRwczpcL1wvd3d3LmxhdmFuZ3VhcmRpYS5jb21cL2NvbWVyXC9yZWNldGFzXC8yMDIwMDMxNVwvNDc0MTA3MjgyMzkxXC9zYW5kd2ljaC1jdWJhbml0by1hc2ktZGUtZmFjaWwtYm9jYWRpbGxvLXNvZmlzdGljYWRvLWV1Z2VuaS5odG1sIiwidGFncyI6WyJzdG9yeSIsIlwvY29tZXIiLCJsYVZhbmd1YXJkaWEiXSwiaXNzIjoidXJuOmxpdmVmeXJlOmdydXBvZ29kbzEuZnlyZS5jbzpzaXRlPTM1MTExMiJ9.xEkF0CqQ', '--', 'FrYz59MxZlHGupJVV4vBDvEGAuM8rtZT8\",\"checkSum\":\"1dc942fcd45564e702eaf236e36fb286\",\"allowComment\":\"allowed\",\"netId\":\"2c017d08', '-', '6515', '-', '11ea-804b-17dc402761eb\",\"networkName\":\"grupogodo1.fyre.co\",\"siteId\":351112', '}', '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', 'Comer', '\\n\\n\\n\\n\\n\\n', 'Recetas', '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', '©', 'La', 'Vanguardia', 'Ediciones', 'Todos', 'lo', 'derecho', 'reservar', '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', 'Quiénes', 'ser', '\\n\\n\\n', 'Contacto', '\\n\\n\\n', 'Aviso', 'legal', '\\n\\n\\n', 'Ayuda', '\\n\\n\\n', 'Política', 'de', 'cookies', '\\n\\n\\n', 'Otras', 'web', '\\n\\n\\n', 'Política', 'de', 'privacidad', '\\n\\n\\n', 'Área', 'de', 'privacidad', '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(visible_text)\n",
    "# Lemmatizing each token\n",
    "mytokens = [word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in doc]\n",
    "print(mytokens) \n",
    "# result: ['he', 'keep', 'eat', 'while', 'we', 'be', 'talk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
