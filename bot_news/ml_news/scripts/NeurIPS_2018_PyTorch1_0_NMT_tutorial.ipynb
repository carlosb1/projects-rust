{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeurIPS_2018_PyTorch1.0_NMT_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlcpBAsMKCRV",
        "colab_type": "code",
        "outputId": "0edb0d4f-88a4-449d-f32d-8cd7612810ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# This will install a preview version of PyTorch 1.0\n",
        "# This version is necessary for some features such as torch.jit.save to work.\n",
        "# This step may take a few minutes.\n",
        "!pip install https://download.pytorch.org/whl/nightly/cu90/torch_nightly-1.0.0.dev20181128-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch-nightly==1.0.0.dev20181128 from https://download.pytorch.org/whl/nightly/cu90/torch_nightly-1.0.0.dev20181128-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/nightly/cu90/torch_nightly-1.0.0.dev20181128-cp36-cp36m-linux_x86_64.whl (592.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 592.0MB 27kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x60faa000 @  0x7f7bb1e4e2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hInstalling collected packages: torch-nightly\n",
            "Successfully installed torch-nightly-1.0.0.dev20181128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6C5cEasKirP",
        "colab_type": "code",
        "outputId": "5c0d1641-7ad4-4c7d-d9a2-83cdf0382607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "# Verify that the version is \"1.0.0.dev20181128\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0.0.dev20181128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLbwscqlrDvW",
        "colab_type": "code",
        "outputId": "e12177da-6177-433f-dbc0-10dbc5cc9e43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "# Fetch IWSLT 2014 German-English data\n",
        "import urllib.request\n",
        "url = \"https://download.pytorch.org/models/translate/iwslt14/data.tar.gz\"\n",
        "local_archive_name = \"data.tar.gz\"\n",
        "urllib.request.urlretrieve(url, local_archive_name)\n",
        "\n",
        "# Extract files.\n",
        "!tar xvzf data.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/\n",
            "data/valid.tok.bpe.en\n",
            "data/valid.tok.de\n",
            "data/train.tok.en\n",
            "data/train.tok.de\n",
            "data/valid.tok.bpe.de\n",
            "data/valid.tok.en\n",
            "data/test.tok.de\n",
            "data/train.tok.bpe.en\n",
            "data/test.tok.bpe.en\n",
            "data/train.tok.bpe.de\n",
            "data/test.tok.bpe.de\n",
            "data/test.tok.en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pssvqLWdmWDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A simple class to induce a vocabulary from a text file.\n",
        "class Dictionary:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.pad_index = 0\n",
        "    self.eos_index = 1\n",
        "    self.unk_index = 2\n",
        "    self.token_indices = {\n",
        "        \"<pad>\": self.pad_index,\n",
        "        \"<eos>\": self.eos_index,\n",
        "        \"<unk>\": self.unk_index,\n",
        "    }\n",
        "    self.tokens = [\"<pad>\",  \"<eos>\",  \"<unk>\"]\n",
        "  \n",
        "  @staticmethod\n",
        "  def induce_from_file(filename, max_size=50000):  \n",
        "    from collections import Counter\n",
        "    text = open(filename).read()\n",
        "    token_counts = Counter(text.split())\n",
        "    \n",
        "    d = Dictionary()\n",
        "    for token, _ in token_counts.most_common(max_size):\n",
        "      d.token_indices[token] = len(d.token_indices)\n",
        "      d.tokens.append(token)\n",
        "\n",
        "    return d\n",
        "      \n",
        "  def get_index(self, token):\n",
        "    return self.token_indices.get(token, self.unk_index)\n",
        "  \n",
        "  def size(self):\n",
        "    return len(self.token_indices)\n",
        "  \n",
        "  def get_token(self, index):\n",
        "    if index > len(self.tokens):\n",
        "      return \"<unk>\"\n",
        "    return self.tokens[index]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CNHfhSEmWHn",
        "colab_type": "code",
        "outputId": "1934e6e1-08c0-4f5d-9fc9-c94d68df27ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "src_dict = Dictionary.induce_from_file(\"data/train.tok.de\")\n",
        "print(\"Loaded source vocabulary of size: \", src_dict.size())\n",
        "trg_dict = Dictionary.induce_from_file(\"data/train.tok.en\")\n",
        "print(\"Loaded target vocabulary of size: \", trg_dict.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded source vocabulary of size:  50003\n",
            "Loaded target vocabulary of size:  50003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9F26zMeKGiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn.utils.rnn import (\n",
        "    pack_padded_sequence,\n",
        "    pad_packed_sequence,\n",
        ")\n",
        "\n",
        "class LstmEncoder(torch.nn.Module):\n",
        "  def __init__(self, embed_dim, hidden_dim, vocab_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed_dim = embed_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    self.embed_tokens = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "    torch.nn.init.uniform_(self.embed_tokens.weight, -0.1, 0.1)\n",
        "\n",
        "    # hidden_dim is combined output dim from both directions\n",
        "    self.lstm = torch.nn.LSTM(\n",
        "        input_size=embed_dim,\n",
        "        hidden_size=hidden_dim // 2,\n",
        "        bidirectional=True,\n",
        "    )\n",
        "    \n",
        "  def forward(self, src_tokens, src_lengths):\n",
        "    \n",
        "    embeddings = self.embed_tokens(src_tokens)\n",
        "    \n",
        "    # Generate packed seq to deal with varying source seq length\n",
        "    # packed_input is of type PackedSequence, which consists of:\n",
        "    # element [0]: a tensor, the packed data, and\n",
        "    # element [1]: a list of integers, the batch size for each step\n",
        "    packed_input = pack_padded_sequence(embeddings, src_lengths)\n",
        "    \n",
        "    packed_output, (_, _) = self.lstm(packed_input)\n",
        "    \n",
        "    #  [max_seqlen, batch_size, hidden_dim]\n",
        "    unpacked_output, _ = pad_packed_sequence(packed_output)\n",
        "    \n",
        "    return unpacked_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDLRnLfYKIp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exercise:\n",
        "# Instantiate an LstmEncoder module and run it on some example input.\n",
        "# Examine the output. Does it have the shapes you expect?\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aMIZ1P28LMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(decoder_state, encoder_outputs):\n",
        "  \"\"\"\n",
        "  decoder_state: trg_len x bsz x dim\n",
        "  encoder_outputs: src_len x bsz x dim\n",
        "  \"\"\"\n",
        "  # bsz x trg_len x dim\n",
        "  decoder_state_t = decoder_state.transpose(0, 1)\n",
        "  # bsz x dim x src_len\n",
        "  encoder_outputs_t = encoder_outputs.permute(1, 2, 0)\n",
        "  # bsz x trg_len x src_len\n",
        "  dot_product = torch.bmm(decoder_state_t, encoder_outputs_t)\n",
        "  # Note: including invalid (padded) positions for code simplicity\n",
        "  norm_dot_product = torch.softmax(dot_product, dim=2)\n",
        "  # bsz x src_len x dim\n",
        "  encoder_outputs_tt = encoder_outputs.transpose(0, 1)\n",
        "  # bsz x trg_len x dim\n",
        "  context = torch.bmm(norm_dot_product, encoder_outputs_tt)\n",
        "  # trg_len x bsz x dim\n",
        "  return context.transpose(0, 1)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhFUId5T_g6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exercise\n",
        "# Create example inputs for the attention function and confirm that it works.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0x7Co1qKI7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LstmDecoder(torch.nn.Module):\n",
        "  def __init__(self, embed_dim, hidden_dim, vocab_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed_dim = embed_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    self.embed_tokens = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "    torch.nn.init.uniform_(self.embed_tokens.weight, -0.1, 0.1)\n",
        "\n",
        "    self.lstm = torch.nn.LSTM(\n",
        "        input_size=embed_dim,\n",
        "        hidden_size=hidden_dim,\n",
        "    )\n",
        "    \n",
        "    # the decoder hidden state is combined with the attention context\n",
        "    # (2 * hidden_dim)\n",
        "    self.output_projection = torch.nn.Linear(2 * hidden_dim, vocab_size)\n",
        "  \n",
        "  def forward(self, input_tokens, encoder_out, prev_state=None):\n",
        "    seqlen, bsz = input_tokens.size()\n",
        "    x = self.embed_tokens(input_tokens)\n",
        "    \n",
        "    if prev_state is None:\n",
        "      h_prev = torch.zeros([1, bsz, self.hidden_dim]).type_as(x)\n",
        "      c_prev = torch.zeros([1, bsz, self.hidden_dim]).type_as(x)\n",
        "    else:\n",
        "      h_prev, c_prev = prev_state\n",
        "\n",
        "    x, (h_next, c_next) = self.lstm(x, (h_prev, c_prev))\n",
        "    \n",
        "    encoder_context = attention(x, encoder_out)\n",
        "    \n",
        "    x = torch.cat([x, encoder_context], dim=2)\n",
        "    \n",
        "    logits = self.output_projection(x)\n",
        "    \n",
        "    return logits, (h_next, c_next)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IKB-52E6Hmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exercise\n",
        "# Use the example output from the encoder exercise, create an example\n",
        "# tensor for input_tokens, and confirm that decoder runs.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeQYPn92LM8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LstmSeq2Seq(torch.nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    encoder_embed_dim,\n",
        "    decoder_embed_dim,\n",
        "    hidden_dim,\n",
        "    src_dict,\n",
        "    trg_dict,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.src_dict = src_dict\n",
        "    self.trg_dict = trg_dict\n",
        "    self.encoder = LstmEncoder(\n",
        "        embed_dim=encoder_embed_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        vocab_size=src_dict.size(),\n",
        "    )\n",
        "    self.decoder = LstmDecoder(\n",
        "        embed_dim=decoder_embed_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        vocab_size=trg_dict.size(),\n",
        "    )\n",
        "  \n",
        "  def forward(self, src_tokens, src_lengths, prev_output_tokens):\n",
        "    encoder_out = self.encoder(src_tokens, src_lengths)\n",
        "    decoder_out = self.decoder(prev_output_tokens, encoder_out)\n",
        "    return decoder_out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dibh_i3jEL7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Corpus():\n",
        "  def __init__(self, src_path, trg_path, src_dict, trg_dict):\n",
        "    self.src_dict = src_dict\n",
        "    self.trg_dict = trg_dict\n",
        "    \n",
        "    self.src_inds = []\n",
        "    for line in open(src_path):\n",
        "      inds = []\n",
        "      for token in line.split():\n",
        "        inds.append(src_dict.get_index(token))\n",
        "      self.src_inds.append(inds)\n",
        "    \n",
        "    self.trg_inds = []\n",
        "    for line in open(trg_path):\n",
        "      inds = []\n",
        "      for token in line.split():\n",
        "        inds.append(trg_dict.get_index(token))\n",
        "      # target sequences need to end with an EOS token\n",
        "      # so that model can predict end of sentence.\n",
        "      inds.append(trg_dict.eos_index)\n",
        "      self.trg_inds.append(inds)\n",
        "      \n",
        "    self.batches = None\n",
        "  \n",
        "  def pad_batch(self, pairs):\n",
        "    \"\"\"\n",
        "    Input pairs is list of 2-tuples (src, trg) where each element is a list\n",
        "    of indices.\n",
        "    Output padded is a list of 3-tuples (src, trg, src_length), which also \n",
        "    includes the original length of the source sentence.\n",
        "    \"\"\"\n",
        "    max_src_len = max(len(src_inds) for (src_inds, _) in pairs)\n",
        "    max_trg_len = max(len(trg_inds) for (_, trg_inds) in pairs)\n",
        "    padded = []\n",
        "    for (src_inds, trg_inds) in pairs:\n",
        "      src_length = len(src_inds)\n",
        "      padded_src_inds = src_inds + ([0] * (max_src_len - len(src_inds)))\n",
        "      padded_trg_inds = trg_inds + ([0] * (max_trg_len - len(trg_inds)))\n",
        "      padded.append((padded_src_inds, padded_trg_inds, src_length))\n",
        "    return padded\n",
        "  \n",
        "  def make_batches(self, batch_size):\n",
        "    pairs = list(zip(self.src_inds, self.trg_inds))\n",
        "    # we batch together similar-length sentence pairs for efficiency\n",
        "    pairs = sorted(\n",
        "        pairs,\n",
        "        key=lambda pair: (len(pair[0]), len(pair[1])),\n",
        "        reverse=True,\n",
        "    )\n",
        "    batches = []\n",
        "    for start_index in range(0, len(pairs), batch_size):\n",
        "      batch = pairs[start_index : start_index + batch_size]\n",
        "      padded_batch = self.pad_batch(batch)\n",
        "      batches.append(padded_batch)\n",
        "    self.batches = batches\n",
        "    \n",
        "  def get_random_batch(self):\n",
        "    return self.batches[np.random.randint(len(self.batches))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NXO3HYpNiF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_batch(model, batch, optimizer, criterion):\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  src_tensor = torch.LongTensor([src_inds for (src_inds, _, _) in batch]).t()\n",
        "  src_lengths = torch.LongTensor([src_length for (_, _, src_length) in batch])\n",
        "  trg_tensor = torch.LongTensor([trg_inds for (_, trg_inds, _) in batch]).t()\n",
        "  \n",
        "  # Decoder inputs begin with EOS\n",
        "  eos = model.trg_dict.eos_index\n",
        "  decoder_input_list = [[eos] + trg_inds[:-1] for (_, trg_inds, _) in batch]\n",
        "  decoder_inputs = torch.LongTensor(decoder_input_list).t()\n",
        "  \n",
        "\n",
        "  logits, _ = model(src_tensor, src_lengths, decoder_inputs)\n",
        "  logits_flat = logits.view(-1, logits.shape[2])\n",
        "  targets_flat = trg_tensor.contiguous().view(-1)\n",
        "\n",
        "  loss = criterion(logits_flat, targets_flat)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28vwJ1HfRETq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq2seq = LstmSeq2Seq(\n",
        "    encoder_embed_dim=128,\n",
        "    decoder_embed_dim=128,\n",
        "    hidden_dim=256,\n",
        "    src_dict=src_dict,\n",
        "    trg_dict=trg_dict,\n",
        ")\n",
        "optimizer = torch.optim.SGD(seq2seq.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss(\n",
        "    ignore_index=trg_dict.pad_index,\n",
        "    reduction='sum',\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMGHLYDdDJY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Excercise\n",
        "# Write a function that will train the model for 3 iterations\n",
        "\n",
        "# As you noticed, training is slow, why?\n",
        "# Exercise: make training work on gpu.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nZ67tgszFCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# After training is complete, verify you can save and load the model\n",
        "# torch.save(seq2seq, \"seq2seq.pt\")\n",
        "# seq2seq_load = torch.load(\"seq2seq.pt\", map_location=\"cpu\")\n",
        "# We're loading to cpu because the rest of the tutorial will deal\n",
        "# with inference on CPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8d6weRBy8cD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BeamStep(torch.nn.Module):\n",
        "  def __init__(self, model, beam_size):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.beam_size = beam_size\n",
        "    \n",
        "  def forward(\n",
        "      self,\n",
        "      encoder_out,\n",
        "      step_input,\n",
        "      prev_scores,\n",
        "      prev_state,\n",
        "  ):\n",
        "    \"\"\"\n",
        "    Applies a single step of beam search.\n",
        "\n",
        "    Inputs:\n",
        "    encoder_out: float tensor of shape, (src_length x beam_size, hidden_dim)\n",
        "    step_input: long tensor of shape (1, beam_size)\n",
        "    prev_scores: float tensor of shape (beam_size)\n",
        "    prev_state: None on first step, tuple (hidden, cell) with elements of shape\n",
        "        (1, beam_size, hidden_dim) thereafter\n",
        "    **note that for input shapes, beam_size is actually 1 in initial step**\n",
        "\n",
        "    Outputs:\n",
        "    best_tokens: long tensor of shape (1, beam_size) [the next step_input]\n",
        "    best_scores: float tensor  of shape (1, beam_size) [the next prev_scores]\n",
        "    next_state: (hidden, cell) each of shape (1, beam_size, hidden_dim)\n",
        "    prev_hypos: long tensor of shape (beam_size)\n",
        "        [indicates which input generated each selected output]\n",
        "    \"\"\"\n",
        "    # logits shape: (1, beam_size, target_vocab_size)\n",
        "    logits, (next_hidden, next_cell) = self.model.decoder(\n",
        "        step_input,\n",
        "        encoder_out,\n",
        "        prev_state,\n",
        "    )\n",
        "    log_probs = torch.log_softmax(logits, dim=2)\n",
        "    # we first select the top beam_size outputs for each input hypothesis \n",
        "    best_scores_k_by_k, best_tokens_k_by_k = torch.topk(\n",
        "      log_probs.squeeze(0),\n",
        "      k=self.beam_size,\n",
        "    )\n",
        "    prev_scores_k_by_k = prev_scores.view(-1, 1).expand(-1, self.beam_size)\n",
        "    total_scores_k_by_k = best_scores_k_by_k + prev_scores_k_by_k\n",
        "    total_scores_flat = total_scores_k_by_k.view(-1)\n",
        "    best_tokens_flat = best_tokens_k_by_k.view(-1)\n",
        "    best_scores, best_indices = torch.topk(total_scores_flat, k=self.beam_size)\n",
        "\n",
        "    best_tokens = best_tokens_flat.index_select(\n",
        "        dim=0,\n",
        "        index=best_indices,\n",
        "    ).view(-1).unsqueeze(0)\n",
        "    # integer division to determine the previous hypothesis from which each best\n",
        "    # token was generated\n",
        "    prev_hypos = best_indices / self.beam_size\n",
        "\n",
        "    next_state = (\n",
        "      next_hidden.index_select(dim=1, index=prev_hypos),\n",
        "      next_cell.index_select(dim=1, index=prev_hypos),\n",
        "    )\n",
        "\n",
        "    return best_tokens, best_scores, next_state, prev_hypos\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQpvR0pRkPBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BeamSearch(torch.jit.ScriptModule):\n",
        "\n",
        "  __constants__ = [\"beam_size\"]\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      model,\n",
        "      beam_size,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.trg_dict = model.trg_dict\n",
        "    self.beam_size = beam_size\n",
        "    \n",
        "    self.eos_index = model.trg_dict.eos_index\n",
        "    self.hidden_dim = model.decoder.hidden_dim\n",
        "\n",
        "    # we trace the encoder computation with example inputs so that\n",
        "    # its Python syntax is not treated as JIT script\n",
        "    example_length = 5\n",
        "    src_tokens = torch.LongTensor(\n",
        "        [model.trg_dict.eos_index] * example_length\n",
        "    ).unsqueeze(1)\n",
        "    src_lengths = torch.LongTensor([model.trg_dict.eos_index])\n",
        "    self.encoder = torch.jit.trace(\n",
        "        model.encoder,\n",
        "        (src_tokens, src_lengths),\n",
        "    )\n",
        "    \n",
        "    encoder_out = model.encoder(src_tokens, src_lengths)\n",
        "    prev_tokens = torch.LongTensor([[model.trg_dict.eos_index]])\n",
        "    prev_scores = torch.FloatTensor([0])\n",
        "    h_prev = torch.zeros([1, 1, model.decoder.hidden_dim])\n",
        "    c_prev = torch.zeros([1, 1, model.decoder.hidden_dim])\n",
        "    prev_state = (h_prev, c_prev)\n",
        "    \n",
        "    beam_step = BeamStep(model, beam_size)\n",
        "    self.beam_step = torch.jit.trace(\n",
        "        beam_step,\n",
        "        (encoder_out, prev_tokens, prev_scores, prev_state),\n",
        "    )\n",
        "  \n",
        "    # tensors cannot be created in-place in a script method\n",
        "    # instead they should be parameters of the torch.jit.ScriptModule\n",
        "    self.init_token = torch.nn.Parameter(\n",
        "        torch.LongTensor([[self.trg_dict.eos_index]]),\n",
        "        requires_grad=False,\n",
        "    )\n",
        "    self.init_score = torch.nn.Parameter(\n",
        "        torch.FloatTensor([0]),\n",
        "        requires_grad=False,\n",
        "    )\n",
        "    self.h_init = torch.nn.Parameter(\n",
        "        torch.zeros([1, 1, model.decoder.hidden_dim]),\n",
        "        requires_grad=False,\n",
        "    )\n",
        "    self.c_init = torch.nn.Parameter(\n",
        "        torch.zeros([1, 1, model.decoder.hidden_dim]),\n",
        "        requires_grad=False,\n",
        "    )\n",
        "  \n",
        "  @torch.jit.script_method\n",
        "  def forward(\n",
        "      self,\n",
        "      src_tokens: torch.Tensor,\n",
        "      src_lengths: torch.Tensor,\n",
        "      num_steps: int,\n",
        "  ):\n",
        "    encoder_out = self.encoder(src_tokens, src_lengths)\n",
        "\n",
        "    prev_tokens, prev_scores, prev_state, prev_hypos = self.beam_step(\n",
        "        encoder_out,\n",
        "        self.init_token,\n",
        "        self.init_score,\n",
        "        (self.h_init, self.c_init),\n",
        "    )\n",
        "\n",
        "    all_tokens = prev_tokens\n",
        "    all_scores = prev_scores.unsqueeze(dim=0)\n",
        "    all_prev_indices = prev_hypos.unsqueeze(dim=0)\n",
        "    \n",
        "    encoder_out = encoder_out.repeat(1, self.beam_size, 1)\n",
        "\n",
        "    for i in range(num_steps - 1):\n",
        "\n",
        "      prev_tokens, prev_scores, prev_state, prev_hypos = self.beam_step(\n",
        "          encoder_out,\n",
        "          prev_tokens,\n",
        "          prev_scores,\n",
        "          prev_state,\n",
        "      )\n",
        "\n",
        "      all_tokens = torch.cat((all_tokens, prev_tokens), dim=0)\n",
        "      all_scores = torch.cat(\n",
        "          (all_scores, prev_scores.unsqueeze(dim=0)),\n",
        "          dim=0,\n",
        "      )\n",
        "      all_prev_indices = torch.cat(\n",
        "          (all_prev_indices, prev_hypos.unsqueeze(dim=0)),\n",
        "          dim=0,\n",
        "      )\n",
        "\n",
        "    return all_tokens, all_scores, all_prev_indices\n",
        "  \n",
        "  @torch.jit.script_method\n",
        "  def get_beam_size(self):\n",
        "    return self.beam_size\n",
        "\n",
        "  def save_to_pytorch(self, output_path):\n",
        "    torch.jit.save(self, output_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gJKfKQ6kPEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "beam_search_module = BeamSearch(seq2seq, 5)\n",
        "# We save the beam search module to PyTorch native format\n",
        "beam_search_module.save_to_pytorch(\"beam_search_net.pytorch_native\")\n",
        "# We can also load the module and run it. For the purpose of this tutorial\n",
        "# we are loading the model in Python but it is also possible to load the model\n",
        "# in C++ to take advantage of the server environment.\n",
        "beam_search_reload = torch.jit.load(\"beam_search_net.pytorch_native\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5EvX6SbzueU",
        "colab_type": "code",
        "outputId": "e12f38f5-36af-4ffd-972d-abb91d039fdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "\n",
        "beam_search_reload"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ScriptModule(\n",
              "  (encoder): ScriptModule(\n",
              "    (embed_tokens): ScriptModule()\n",
              "    (lstm): ScriptModule()\n",
              "  )\n",
              "  (beam_step): ScriptModule(\n",
              "    (model): ScriptModule(\n",
              "      (encoder): ScriptModule(\n",
              "        (embed_tokens): ScriptModule()\n",
              "        (lstm): ScriptModule()\n",
              "      )\n",
              "      (decoder): ScriptModule(\n",
              "        (embed_tokens): ScriptModule()\n",
              "        (lstm): ScriptModule()\n",
              "        (output_projection): ScriptModule()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDbxDyJ9naCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode(input_sentence, beam_search_module, src_dict, trg_dict, num_steps):\n",
        "  \"\"\"The forward function of the BeamSearch module generated a set of\n",
        "  hypotheses. We now need to extract the best hypothesis.\"\"\"\n",
        "  indices = [src_dict.get_index(token) for token in input_sentence.split()]\n",
        "  src_tokens = torch.LongTensor(indices).unsqueeze(1)\n",
        "  src_length = torch.LongTensor([len(indices)])\n",
        "  \n",
        "  tokens, scores, prev_indices = beam_search_module(\n",
        "      src_tokens,\n",
        "      src_length,\n",
        "      num_steps,\n",
        "  )\n",
        "  \n",
        "  best_score = scores[-1, 0] / num_steps\n",
        "  best_hypo = num_steps - 1, 0\n",
        "  \n",
        "  for i in range(num_steps - 1):\n",
        "    for j in range(beam_search_module.get_beam_size()):\n",
        "      score = scores[i, j] / (i + 1)\n",
        "      if score > best_score:\n",
        "        best_score = score\n",
        "        best_hypo = (i, j)\n",
        "  \n",
        "  indices = []\n",
        "  for s in range(best_hypo[0], -1, -1):\n",
        "    indices.append(tokens[best_hypo])\n",
        "    best_hypo = s - 1, prev_indices[best_hypo]\n",
        "  \n",
        "  indices.reverse()\n",
        "  if trg_dict.eos_index in indices:\n",
        "    indices = indices[:indices.index(trg_dict.eos_index)]\n",
        "\n",
        "  if len(indices) == 0:\n",
        "    return input_sentence\n",
        "  \n",
        "  output = \" \".join([trg_dict.get_token(ind) for ind in indices])\n",
        "    \n",
        "  print(output)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeH3pqyGn-fI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decode(\"meine freunde\", beam_search_module, src_dict, trg_dict, 5)\n",
        "# Note that at this point, seq2seq has not been trained much so the output\n",
        "# is quite random.\n",
        "# This is an example random output: \"skyscraper handling redefines basses handling\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-k5cBO1qgKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we provide a pretrained model for download\n",
        "\n",
        "import urllib\n",
        "url = \"https://download.pytorch.org/models/translate/iwslt14/neurips_tutorial_seq2seq.pt\"\n",
        "pretrained_model_name = \"neurips_tutorial_seq2seq.pt\"\n",
        "urllib.request.urlretrieve(url, pretrained_model_name)\n",
        "\n",
        "seq2seq_pretrained = LstmSeq2Seq(\n",
        "    encoder_embed_dim=128,\n",
        "    decoder_embed_dim=128,\n",
        "    hidden_dim=256,\n",
        "    src_dict=src_dict,\n",
        "    trg_dict=trg_dict,\n",
        ")\n",
        "seq2seq_pretrained.load_state_dict(torch.load(pretrained_model_name))\n",
        "beam_search_pretrained = BeamSearch(seq2seq_pretrained, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhyxcAsCJYYj",
        "colab_type": "code",
        "outputId": "7452025e-ab14-4b00-c21b-7e9838945104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# You can try feeding running the model through a few German sentence\n",
        "# Note that the training data was lowercased in preprocessing so it is better to\n",
        "# provide lowercase input.\n",
        "decode(\"meine freunde\", beam_search_pretrained, src_dict, trg_dict, 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my friends\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3WxTLU3Jn4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exercise\n",
        "# Modify inference to sample 25 output sentences instead of using beam search\n",
        "# Note: This will be easier using the original trained PyTorch model!\n",
        "#       Creating a fast inference model via torch.jit.save is left as an\n",
        "#       advanced exercise."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3OCFvAsdacW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check out https://github.com/pytorch/translate for more advanced features!\n",
        "# Thanks for participating in this tutorial :)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}